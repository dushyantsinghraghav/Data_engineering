{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dushyantsinghraghav/Data_engineering/blob/main/ELT_using_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0AcHtge84uT"
      },
      "source": [
        "   # ASSIGNMENT 2B\n",
        "   # DUSHYANT SINGH RAGHAV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2562DXS884uW"
      },
      "source": [
        "# Streaming application using Spark Structured Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB1efi1m84uX"
      },
      "source": [
        "# Creating the spark session with two processors\n",
        "# 2.1\n",
        "Importing pyspark and creating spark session .\n",
        "we create the dataschema for pedestrian count and sensor location csv files similar to asssignment 2A\\\n",
        "we will use only two local processors instead of using all the process"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APSvcFrL9N5Z",
        "outputId": "06234abf-3dd1-4f10-e078-b0af9875032e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=a81cf7f28c7bfaf3b620a4173491bd79ea12a0ebd75b561528084e5b95314483\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI5NYevN84uY",
        "outputId": "aca72715-0389-441f-ee7a-1a9fac4940de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the first part is implemented\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pyspark\n",
        "from pyspark import SparkConf\n",
        "master=\"local[2]\"\n",
        "app_name=\"Assignment 2B solution\"\n",
        "spark_conf=SparkConf().setMaster(master).setAppName(app_name)\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "#getting or instatiating spark context object\n",
        "sc=SparkContext.getOrCreate(spark_conf)\n",
        "sc.setLogLevel('ERROR')\n",
        "#we create spark object\n",
        "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
        "print(\"the first part is implemented\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYKkzu7r84uZ"
      },
      "source": [
        "# Creating data schema for pedestrian count dataframe\n",
        "# 2.2\n",
        "Defining the data schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "TfwVCpOS84ua",
        "outputId": "349f7355-c581-4854-c3c7-0df34284cde3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "StructType([StructField('ID', IntegerType(), True), StructField('Date_Time', StringType(), True), StructField('Year', IntegerType(), True), StructField('Month', StringType(), True), StructField('Mdate', IntegerType(), True), StructField('Day', StringType(), True), StructField('Time', IntegerType(), True), StructField('Sensor_ID', IntegerType(), True), StructField('Sensor_Name', StringType(), True), StructField('Hourly_Counts', IntegerType(), True)])"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import to_timestamp, date_format\n",
        "spark.conf.set('spark.sql.session.timeZone', 'UTC')\n",
        "\n",
        "\n",
        "\n",
        "# define  data types for schema for two files pedestrian count and sensor location\n",
        "pd_count_schema = StructType([\n",
        "StructField(\"ID\", IntegerType(), True),\n",
        "StructField(\"Date_Time\", StringType(), True),\n",
        "StructField(\"Year\", IntegerType(), True),\n",
        "StructField(\"Month\", StringType(), True),\n",
        "StructField(\"Mdate\", IntegerType(), True),\n",
        "StructField(\"Day\",StringType(),True),\n",
        "StructField(\"Time\", IntegerType(), True),\n",
        "StructField(\"Sensor_ID\", IntegerType(), True),\n",
        "StructField(\"Sensor_Name\", StringType(), True),\n",
        "StructField(\"Hourly_Counts\", IntegerType(), True)\n",
        "])\n",
        "display(pd_count_schema)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drq6n3Lh84ua"
      },
      "source": [
        "# Defining the dataschema for sensor location\n",
        "In this we define the data schema for sensor location csv and then read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Vpqs7uME84ub",
        "outputId": "594556ff-ba88-476b-d0ab-aed37a185b14"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "StructType([StructField('sensor_id', IntegerType(), True), StructField('sensor_name', StringType(), True), StructField('sensor_description', StringType(), True), StructField('installation_date', StringType(), True), StructField('status', StringType(), True), StructField('note', StringType(), True), StructField('direction_1', StringType(), True), StructField('direction_2', StringType(), True), StructField('latitude', FloatType(), True), StructField('longitude', FloatType(), True), StructField('location', StringType(), True)])"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#similarly defining the schema for the sensor location file\n",
        "sensor_location_schema=StructType([\n",
        "StructField(\"sensor_id\", IntegerType(), True),\n",
        "StructField(\"sensor_name\", StringType(), True),\n",
        "StructField(\"sensor_description\", StringType(), True),\n",
        "StructField(\"installation_date\", StringType(), True),\n",
        "StructField(\"status\", StringType(), True),\n",
        "StructField(\"note\", StringType(), True),\n",
        "StructField(\"direction_1\", StringType(), True),\n",
        "StructField(\"direction_2\", StringType(), True),\n",
        "StructField(\"latitude\", FloatType(), True),\n",
        "StructField(\"longitude\", FloatType(), True),\n",
        "StructField(\"location\", StringType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "display(sensor_location_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5ZGG5bh84ub"
      },
      "source": [
        "# we load the respective data schema above to create independent dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I4r9_HY84uc",
        "outputId": "f5629630-723f-4a8c-f211-1dcdd6a13af3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+----+--------+-----+-------+----+---------+--------------------+-------------+\n",
            "|     ID|           Date_Time|Year|   Month|Mdate|    Day|Time|Sensor_ID|         Sensor_Name|Hourly_Counts|\n",
            "+-------+--------------------+----+--------+-----+-------+----+---------+--------------------+-------------+\n",
            "|3435630|12/01/2020 08:00:...|2020|December|    1|Tuesday|   8|       39|        Alfred Place|           83|\n",
            "|3435798|12/01/2020 11:00:...|2020|December|    1|Tuesday|  11|       12|            New Quay|           86|\n",
            "|3435107|12/01/2020 12:00:...|2020|December|    1|Tuesday|   0|        4|    Town Hall (West)|           78|\n",
            "|3435108|12/01/2020 12:00:...|2020|December|    1|Tuesday|   0|       17|Collins Place (So...|           10|\n",
            "|3435109|12/01/2020 12:00:...|2020|December|    1|Tuesday|   0|       18|Collins Place (No...|            6|\n",
            "|3435110|12/01/2020 12:00:...|2020|December|    1|Tuesday|   0|       53|  Collins St (North)|           18|\n",
            "|3435111|12/01/2020 12:00:...|2020|December|    1|Tuesday|   0|        2|Bourke Street Mal...|           26|\n",
            "|3435112|12/01/2020 12:00:...|2020|December|    1|Tuesday|   0|        1|Bourke Street Mal...|           52|\n",
            "|3435114|12/01/2020 12:00:...|2020|December|    1|Tuesday|   0|        9|Southern Cross St...|            6|\n",
            "|3435115|12/01/2020 12:00:...|2020|December|    1|Tuesday|   0|       10|      Victoria Point|            0|\n",
            "+-------+--------------------+----+--------+-----+-------+----+---------+--------------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+---------+--------------------+------------------+-----------------+------+----+-----------+-----------+----------+---------+--------------------+\n",
            "|sensor_id|         sensor_name|sensor_description|installation_date|status|note|direction_1|direction_2|  latitude|longitude|            location|\n",
            "+---------+--------------------+------------------+-----------------+------+----+-----------+-----------+----------+---------+--------------------+\n",
            "|       59|    Building 80 RMIT|            RMIT_T|       2019/02/13|     A|null|      North|      South|-37.808258|144.96304|(-37.80825648, 14...|\n",
            "|       20|Chinatown-Lt Bour...|          LtB170_T|       2013/09/06|     A|null|       East|       West| -37.81173|144.96825|(-37.81172913, 14...|\n",
            "|       34|Flinders St-Spark La|           Fli32_T|       2014/06/08|     A|null|       East|       West| -37.81538|144.97415|(-37.81537985, 14...|\n",
            "|       57|    Bourke St Bridge|          BouBri_T|       2018/08/13|     A|null|       West|       East|-37.817673|144.95026|(-37.8176735, 144...|\n",
            "|       40|Lonsdale St-Sprin...|          Spr201_T|       2015/01/19|     A|null|      South|      North|-37.809994|144.97227|(-37.80999341, 14...|\n",
            "|       31|     Lygon St (West)|          Lyg161_T|       2013/10/10|     A|null|      South|      North|-37.801697|144.96658|(-37.8016968, 144...|\n",
            "|       19|Chinatown-Swansto...|          LtB210_T|       2013/09/02|     A|null|       East|       West| -37.81237| 144.9655|(-37.81237202, 14...|\n",
            "|       37|     Lygon St (East)|          Lyg260_T|       2015/02/11|     A|null|      North|      South|-37.803104|144.96672|(-37.80310271, 14...|\n",
            "|       36|     Queen St (West)|           Que85_T|       2015/01/20|     A|null|      South|      North|-37.816525|144.96121|(-37.81652527, 14...|\n",
            "+---------+--------------------+------------------+-----------------+------+----+-----------+-----------+----------+---------+--------------------+\n",
            "only showing top 9 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#now we load the respective schema files and create the respective dataframes\n",
        "from pyspark.sql import functions as F\n",
        "pd_count= spark.read.schema(pd_count_schema).options(header=True,inferSchema=True,multiline=True,delimiter=\",\",mode=\"PERMISSIVE\").csv(\"/content/Streaming_Pedestrian_December_counts_per_hour.csv\")\n",
        "pd_count.show(10)\n",
        "sensor_location_count=spark.read.schema(sensor_location_schema).options(header=True,inferSchema=True,multiline=True,delimiter=\",\",mode=\"PERMISSIVE\").csv(\"Pedestrian_Counting_System_-_Sensor_Locations.csv\")\n",
        "sensor_location_count.show(9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfZThWm_84uc",
        "outputId": "eac026aa-e6e1-4be6-c413-d05401376af3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+----+-----+-----+---+----+---------+-----------+-------------+\n",
            "| ID|Date_Time|Year|Month|Mdate|Day|Time|Sensor_ID|Sensor_Name|Hourly_Counts|\n",
            "+---+---------+----+-----+-----+---+----+---------+-----------+-------------+\n",
            "+---+---------+----+-----+-----+---+----+---------+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pd_new=pd_count.filter((pd_count.Time<24) & (pd_count.Time>23))\n",
        "pd_new.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIA3VShQ84uc"
      },
      "outputs": [],
      "source": [
        "#now we stop the above spark session\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9LY_3LE84uc"
      },
      "source": [
        "# Now starting  the spark session for kafka process\n",
        "# 2.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqx8mlC284ud"
      },
      "outputs": [],
      "source": [
        "#in this  this we first read the stream of data sent by the producer and for this we create\n",
        "import os\n",
        "os.environ['PYSPARK_SUBMIT_ARGS']='--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode,split\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "spark1=SparkSession\\\n",
        "   .builder\\\n",
        "   .appName(\"pedesrtian data anlaysis\")\\\n",
        "   .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEICUXQL84ud"
      },
      "outputs": [],
      "source": [
        "#we subscribe to the pedestrian analysis using spark1\n",
        "topic='pedestrian_analysis'\n",
        "df=spark1\\\n",
        "   .readStream\\\n",
        "   .format(\"kafka\")\\\n",
        "   .option(\"kafka.bootstrap.servers\",\"127.0.0.1:9092\")\\\n",
        "   .option(\"subscribe\",topic)\\\n",
        "   .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcGxkzsz84ud",
        "outputId": "6c21fbf2-e371-4f60-d6fb-c04422446fc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- key: binary (nullable = true)\n",
            " |-- value: binary (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- partition: integer (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- timestampType: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#now defining the schema for the readstream from the above topic\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0frNtae84ud"
      },
      "outputs": [],
      "source": [
        "df=df.selectExpr( \"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frKyzksy84ue",
        "outputId": "9aceba36-bf3f-4f27-ab5a-e29dd68c97fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- key: string (nullable = true)\n",
            " |-- value: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#query.stop()\n",
        "df.printSchema()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCja8vuf84ue"
      },
      "source": [
        "\n",
        "# we define the schema  for the streaming data\n",
        "here we define the schema for the pedesrtian count producer file and then read it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeUA93rO84ue"
      },
      "outputs": [],
      "source": [
        "#using explode to get the several columns\n",
        "from pyspark.sql.types import *\n",
        "pd_count_schema = ArrayType(StructType([\n",
        "\n",
        "StructField(\"Day\",StringType(),True),\n",
        "StructField(\"Hourly_Counts\", IntegerType(), True),\n",
        "StructField(\"Mdate\",IntegerType(),True),\n",
        "StructField(\"Date_Time\",StringType(),True),\n",
        "StructField(\"Time\",IntegerType(),True),\n",
        "StructField(\"Sensor_Name\",StringType(),True),\n",
        "StructField(\"ID\",StringType(),True),\n",
        "StructField(\"Year\",IntegerType(),True),\n",
        "StructField(\"Sensor_ID\",IntegerType(),True),\n",
        "StructField(\"Month\",StringType(),True),\n",
        "StructField(\"ts\",TimestampType(),True)\n",
        "]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylHnm7GS84ue"
      },
      "outputs": [],
      "source": [
        "df = df.select(F.from_json(F.col(\"value\").cast(\"string\"), pd_count_schema).alias('parsed_value'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOl-NgsY84ue",
        "outputId": "b70e31da-00d1-4d48-a2db-d3e3615f6f5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- parsed_value: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- Day: string (nullable = true)\n",
            " |    |    |-- Hourly_Counts: integer (nullable = true)\n",
            " |    |    |-- Mdate: integer (nullable = true)\n",
            " |    |    |-- Date_Time: string (nullable = true)\n",
            " |    |    |-- Time: integer (nullable = true)\n",
            " |    |    |-- Sensor_Name: string (nullable = true)\n",
            " |    |    |-- ID: string (nullable = true)\n",
            " |    |    |-- Year: integer (nullable = true)\n",
            " |    |    |-- Sensor_ID: integer (nullable = true)\n",
            " |    |    |-- Month: string (nullable = true)\n",
            " |    |    |-- ts: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI8IlPRr84ue"
      },
      "outputs": [],
      "source": [
        "df = df.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Hakz8rO84uf",
        "outputId": "704215f2-d2d5-487d-a5f3-f257da564d32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- unnested_value: struct (nullable = true)\n",
            " |    |-- Day: string (nullable = true)\n",
            " |    |-- Hourly_Counts: integer (nullable = true)\n",
            " |    |-- Mdate: integer (nullable = true)\n",
            " |    |-- Date_Time: string (nullable = true)\n",
            " |    |-- Time: integer (nullable = true)\n",
            " |    |-- Sensor_Name: string (nullable = true)\n",
            " |    |-- ID: string (nullable = true)\n",
            " |    |-- Year: integer (nullable = true)\n",
            " |    |-- Sensor_ID: integer (nullable = true)\n",
            " |    |-- Month: string (nullable = true)\n",
            " |    |-- ts: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c47XVVaw84uf"
      },
      "outputs": [],
      "source": [
        "query = df\\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi7xakLW84uf"
      },
      "outputs": [],
      "source": [
        "query.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ6asV9N84uf"
      },
      "source": [
        "# writing the raw data onto the parquet file and it is important for real time data\n",
        "# 2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm1uHXNl84uf"
      },
      "outputs": [],
      "source": [
        "#writing the created dataframe to the parquet file\n",
        "query_file_sink = df.writeStream.format(\"parquet\")\\\n",
        "        .outputMode(\"append\")\\\n",
        "        .option(\"path\", \"parquet/pedestrian_raw_df\")\\\n",
        "        .option(\"checkpointLocation\", \"parquet/pedestrian_raw_df/checkpoint\")\\\n",
        "        .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzCvyVf984uf"
      },
      "outputs": [],
      "source": [
        "query_file_sink.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6xJDqdm84uf",
        "outputId": "14092f8c-5eaf-4ce3-b12c-3de55429fb46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- unnested_value: struct (nullable = true)\n",
            " |    |-- Day: string (nullable = true)\n",
            " |    |-- Hourly_Counts: integer (nullable = true)\n",
            " |    |-- Mdate: integer (nullable = true)\n",
            " |    |-- Date_Time: string (nullable = true)\n",
            " |    |-- Time: integer (nullable = true)\n",
            " |    |-- Sensor_Name: string (nullable = true)\n",
            " |    |-- ID: string (nullable = true)\n",
            " |    |-- Year: integer (nullable = true)\n",
            " |    |-- Sensor_ID: integer (nullable = true)\n",
            " |    |-- Month: string (nullable = true)\n",
            " |    |-- ts: timestamp (nullable = true)\n",
            "\n",
            "+--------------------+\n",
            "|      unnested_value|\n",
            "+--------------------+\n",
            "|[Tuesday, 3, 1, 1...|\n",
            "|[Tuesday, 27, 1, ...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#now reading the parquet file\n",
        "query_file_sink_df = spark1.read.parquet(\"parquet/pedestrian_raw_df\")\n",
        "query_file_sink_df.printSchema()\n",
        "query_file_sink_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3XmHvGW84ug",
        "outputId": "6e749068-f9e1-45ec-da4e-805060a7ff8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- unnested_value: struct (nullable = true)\n",
            " |    |-- Day: string (nullable = true)\n",
            " |    |-- Hourly_Counts: integer (nullable = true)\n",
            " |    |-- Mdate: integer (nullable = true)\n",
            " |    |-- Date_Time: string (nullable = true)\n",
            " |    |-- Time: integer (nullable = true)\n",
            " |    |-- Sensor_Name: string (nullable = true)\n",
            " |    |-- ID: string (nullable = true)\n",
            " |    |-- Year: integer (nullable = true)\n",
            " |    |-- Sensor_ID: integer (nullable = true)\n",
            " |    |-- Month: string (nullable = true)\n",
            " |    |-- ts: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm5T-x1I84ug"
      },
      "source": [
        "# Transforming streaming data into proper format\n",
        "# 2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAe4bN9M84ug"
      },
      "outputs": [],
      "source": [
        "df_formatted = df.select(\n",
        "                    F.col(\"unnested_value.Day\").alias(\"Day\"),\n",
        "                    F.col(\"unnested_value.Hourly_Counts\").alias(\"Hourly_Counts\"),\n",
        "                    F.col(\"unnested_value.Mdate\").alias(\"Mdate\"),\n",
        "                    F.col(\"unnested_value.Date_Time\").alias(\"Date_Time\"),\n",
        "                    F.col(\"unnested_value.Time\").alias(\"Time\"),\n",
        "                    F.col(\"unnested_value.Sensor_Name\").alias(\"Sensor_Name\"),\n",
        "                    F.col(\"unnested_value.ID\").alias(\"ID\"),\n",
        "                    F.col(\"unnested_value.Year\").alias(\"Year\"),\n",
        "                    F.col(\"unnested_value.Sensor_ID\").alias(\"Sensor_ID\"),\n",
        "                    F.col(\"unnested_value.Month\").alias(\"Month\"),\n",
        "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUtgb_z284ug",
        "outputId": "278f7cb4-5d2b-404e-a8c6-e4cc68058278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Day: string (nullable = true)\n",
            " |-- Hourly_Counts: integer (nullable = true)\n",
            " |-- Mdate: integer (nullable = true)\n",
            " |-- Date_Time: string (nullable = true)\n",
            " |-- Time: integer (nullable = true)\n",
            " |-- Sensor_Name: string (nullable = true)\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Sensor_ID: integer (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- ts: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_formatted.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5vioewr84ug"
      },
      "outputs": [],
      "source": [
        "#this query is used to check whether the values of the column are visible in the dataframe\n",
        "#df_formatted is the name of the dataframe which we get after structure definition\n",
        "query = df_formatted\\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82cl-QlA84ug"
      },
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AspXPlA84uh",
        "outputId": "e25f48fa-d4e3-47ab-f134-901c59f7be87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Day: string (nullable = true)\n",
            " |-- Hourly_Counts: integer (nullable = true)\n",
            " |-- Mdate: integer (nullable = true)\n",
            " |-- Date_Time: string (nullable = true)\n",
            " |-- Time: integer (nullable = true)\n",
            " |-- Sensor_Name: string (nullable = true)\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Sensor_ID: integer (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- ts: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_formatted.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naECX6FY84uh"
      },
      "outputs": [],
      "source": [
        "#query to display the original dataframe values on console\n",
        "query = df_formatted \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5kR4fgn84uh"
      },
      "outputs": [],
      "source": [
        "#command to stop the query\n",
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cagTKL2I84uk"
      },
      "source": [
        "# creating new columns next_date,next_Mdate,next_day_week,prev_count,\n",
        "# next_day_of_week\n",
        "# 2.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5eplCcb84ul"
      },
      "outputs": [],
      "source": [
        "#combining the data of each day in one batch and then writing it to the console\n",
        "#converting\n",
        "from pyspark.sql.functions import split,col,date_add,to_date,dayofweek,to_timestamp\n",
        "\n",
        "df2=df_formatted.withColumn(\"Date_Time\",to_date(\"Date_Time\",'MM/dd/yyyy hh:mm:ss a'))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLZ2BLtu84ul",
        "outputId": "f847d0c1-5b6a-437e-ee25-d53c239f5085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Day: string (nullable = true)\n",
            " |-- Hourly_Counts: integer (nullable = true)\n",
            " |-- Mdate: integer (nullable = true)\n",
            " |-- Date_Time: date (nullable = true)\n",
            " |-- Time: integer (nullable = true)\n",
            " |-- Sensor_Name: string (nullable = true)\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Sensor_ID: integer (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- ts: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#printing the dataschema\n",
        "df2.printSchema()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrZoVjKx84ul"
      },
      "outputs": [],
      "source": [
        "#filtering the data\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import DateType,IntegerType\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import dayofweek,to_timestamp,date_format,next_day,dayofyear,weekofyear,dayofmonth\n",
        "\n",
        "#now we create new columns next_Mdate,prev_count,next_date,next_day_week and next_day_of_week\n",
        "#using withColumn and we apply all the operations on all the df2 dataframe\n",
        "#df2 is the final dataframe after all the operations\n",
        "\n",
        "df2=df2.withColumn(\"prev_count\",col(\"Hourly_Counts\"))\n",
        "df2=df2.withColumn(\"next_date\",date_add(df2.Date_Time, 1))\n",
        "df2=df2.withColumn(\"next_day_of_week\",dayofweek(col('next_date')))\n",
        "df2=df2.withColumn(\"next_day_week\",weekofyear(col('next_date')))\n",
        "df2=df2.withColumn(\"next_Mdate\",dayofmonth(col(\"next_date\")))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwMJlR3n84ul"
      },
      "outputs": [],
      "source": [
        "#filtering the dataframe df4 based on time constraint of 9 until 11:55\n",
        "#after applying the time fiilter on df2 dataframe we get df6 dataframe\n",
        "#which is used for pipelinemodel\n",
        "df6=df2.filter((col('Time')>=9) &(col('Time')<=23))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWKMU3Nt84ul",
        "outputId": "1fbc02f2-5559-48f1-91a9-bc8f0b1262e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Day: string (nullable = true)\n",
            " |-- Hourly_Counts: integer (nullable = true)\n",
            " |-- Mdate: integer (nullable = true)\n",
            " |-- Date_Time: date (nullable = true)\n",
            " |-- Time: integer (nullable = true)\n",
            " |-- Sensor_Name: string (nullable = true)\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Sensor_ID: integer (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- ts: timestamp (nullable = true)\n",
            " |-- prev_count: integer (nullable = true)\n",
            " |-- next_date: date (nullable = true)\n",
            " |-- next_day_of_week: integer (nullable = true)\n",
            " |-- next_day_week: integer (nullable = true)\n",
            " |-- next_Mdate: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- Day: string (nullable = true)\n",
            " |-- Hourly_Counts: integer (nullable = true)\n",
            " |-- Mdate: integer (nullable = true)\n",
            " |-- Date_Time: date (nullable = true)\n",
            " |-- Time: integer (nullable = true)\n",
            " |-- Sensor_Name: string (nullable = true)\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Sensor_ID: integer (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- ts: timestamp (nullable = true)\n",
            " |-- prev_count: integer (nullable = true)\n",
            " |-- next_date: date (nullable = true)\n",
            " |-- next_day_of_week: integer (nullable = true)\n",
            " |-- next_day_week: integer (nullable = true)\n",
            " |-- next_Mdate: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#datatSchema of different dataframes\n",
        "#we have two dataframes df2 and df6\n",
        "\n",
        "df2.printSchema()\n",
        "\n",
        "df6.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-xCP4Qh84um"
      },
      "outputs": [],
      "source": [
        "#Loadding the dataframe onto the console after creating new columns\n",
        "#the dataframe we get is df2\n",
        "query = df2\\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-N7q20784um"
      },
      "outputs": [],
      "source": [
        "#command to stop the query\n",
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqaaqRAU84um"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X-0YShV84um"
      },
      "source": [
        "# Loading the pipeline and then using it for tranformation and estimation\n",
        "# 2.7\n",
        "we load the pipeline by imprting the PipelineModel from pyspark.ml\n",
        "and then we tranform the pedestrian count dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmSPqR7N84um"
      },
      "outputs": [],
      "source": [
        "#loading the pipeline model\n",
        "from pyspark.ml import PipelineModel\n",
        "PipelineModel=PipelineModel.load(\"count_estimation_pipeline_model\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpQ3P8c484um"
      },
      "outputs": [],
      "source": [
        "#now we apply the pipeline model tranformation and estimation on the train and test data\n",
        "#we get the predict dataframe after transformation of df6 dataframe\n",
        "#moreover pipeline is applied on df2 dataframe\n",
        "\n",
        "predict=PipelineModel.transform(df6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-w-8Cjr84un"
      },
      "outputs": [],
      "source": [
        "#we select the respective columns such as prev_count,prediction,Time,month for pr dataframe\n",
        "pr=predict.select('prev_count','prediction','next_day_week','Day')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUp_NjQu84un",
        "outputId": "528aed46-c195-4073-a5f9-757a266e520f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Day: string (nullable = true)\n",
            " |-- Hourly_Counts: integer (nullable = true)\n",
            " |-- Mdate: integer (nullable = true)\n",
            " |-- Date_Time: date (nullable = true)\n",
            " |-- Time: integer (nullable = true)\n",
            " |-- Sensor_Name: string (nullable = true)\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Sensor_ID: integer (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- ts: timestamp (nullable = true)\n",
            " |-- prev_count: integer (nullable = true)\n",
            " |-- next_date: date (nullable = true)\n",
            " |-- next_day_of_week: integer (nullable = true)\n",
            " |-- next_day_week: integer (nullable = true)\n",
            " |-- next_Mdate: integer (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- prediction: double (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predict.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd18opnl84un"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbTP7eIl84un"
      },
      "outputs": [],
      "source": [
        "query = pr\\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivEVSF4X84un"
      },
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w34XjmgZ84un"
      },
      "source": [
        "# writing the pr dataframe onto the parquet which includes predictions\n",
        "# after applying the pipeline\n",
        "# 2.7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZQvJSw484uo"
      },
      "outputs": [],
      "source": [
        "query_file_sink_pr = predict.writeStream.format(\"parquet\")\\\n",
        "        .outputMode(\"append\")\\\n",
        "        .option(\"path\", \"parquet/prediction_df\")\\\n",
        "        .option(\"checkpointLocation\", \"parquet/prediction_df/checkpoint\")\\\n",
        "        .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM7jhiSn84uo"
      },
      "outputs": [],
      "source": [
        "query_file_sink_pr.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsnvJC5Z84uo",
        "outputId": "9cdb85e8-d36f-40f9-a5da-7ade2c1960c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Day: string (nullable = true)\n",
            " |-- Hourly_Counts: string (nullable = true)\n",
            " |-- Mdate: integer (nullable = true)\n",
            " |-- Date_Time: date (nullable = true)\n",
            " |-- Time: integer (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Month: string (nullable = true)\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Sensor_ID: integer (nullable = true)\n",
            " |-- Sensor_Name: string (nullable = true)\n",
            " |-- ts: timestamp (nullable = true)\n",
            " |-- next_Mdate: integer (nullable = true)\n",
            " |-- prev_count: integer (nullable = true)\n",
            " |-- next_date: date (nullable = true)\n",
            " |-- next_day_week: integer (nullable = true)\n",
            " |-- next_day_of_week: integer (nullable = true)\n",
            "\n",
            "+--------+-------------+-----+----------+----+----+--------+-------+---------+--------------------+-------------------+----------+----------+----------+-------------+----------------+\n",
            "|     Day|Hourly_Counts|Mdate| Date_Time|Time|Year|   Month|     ID|Sensor_ID|         Sensor_Name|                 ts|next_Mdate|prev_count| next_date|next_day_week|next_day_of_week|\n",
            "+--------+-------------+-----+----------+----+----+--------+-------+---------+--------------------+-------------------+----------+----------+----------+-------------+----------------+\n",
            "|Saturday|          548|    5|2020-12-05|  14|2020|December|3442039|       69|Flinders Ln -Degr...|2021-02-10 13:20:23|         6|       548|2020-12-06|            1|             341|\n",
            "|Saturday|          490|    5|2020-12-05|  14|2020|December|3442038|       68|Flinders Ln -Degr...|2021-02-10 13:20:22|         6|       490|2020-12-06|            1|             341|\n",
            "|Saturday|           12|    5|2020-12-05|  14|2020|December|3442041|       71|      Westwood Place|2021-02-10 13:20:24|         6|        12|2020-12-06|            1|             341|\n",
            "+--------+-------------+-----+----------+----+----+--------+-------+---------+--------------------+-------------------+----------+----------+----------+-------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#reading the parquet file\n",
        "query_file_sink_df_pr = spark1.read.parquet(\"parquet/prediction_df\")\n",
        "query_file_sink_df_pr.printSchema()\n",
        "query_file_sink_df_pr.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jaV90bJ84uo"
      },
      "source": [
        "\n",
        "# window creation\n",
        "# 2.8.a\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gOybM4A84uo"
      },
      "outputs": [],
      "source": [
        "#now using watermarking to get the Hourly_Counts\n",
        "#using tolerance time and window time of 10 seconds\n",
        "from pyspark.sql.functions import *\n",
        "windowedCounts = predict \\\n",
        "    .groupBy(window(predict.Day,\"5 seconds\"),predict.Sensor_ID,predict.Hourly_Counts)\\\n",
        "    .agg(F.count(col(\"Time\")).alias(\"total\"))\\\n",
        "    .select(\"window\",\"total\",\"Sensor_ID\",\"Hourly_Counts\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJltTz5L84up"
      },
      "outputs": [],
      "source": [
        "#creating a query to display the data on console\n",
        "query_window= windowedCounts \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"sensor_hourly_counts\")\\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-Ci0glg84up",
        "outputId": "fc0c0578-bfda-481f-9627-5c04f3172e26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----+---------+-------------+\n",
            "|window|total|Sensor_ID|Hourly_Counts|\n",
            "+------+-----+---------+-------------+\n",
            "+------+-----+---------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#spark query to display the count of no of hours where Hourly counts >2000 including the sensor id\n",
        "spark1.sql(\"select * from sensor_hourly_counts \").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3_66Dlb84up"
      },
      "outputs": [],
      "source": [
        "#to display the no of hours for each sensor where the Hourly_Count was greater than 2000 using the sql command\n",
        "\n",
        "query_window.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txniXlcB84up"
      },
      "outputs": [],
      "source": [
        "#2.8.b to display the sensor longitude and latitude along with the  details of Hourly_Count>2000 on next date\n",
        "#here we need to join the two dataframes on common attribute and that is sensor_id\n",
        "topic='sensor_location'\n",
        "df_sensor=spark1\\\n",
        "   .readStream\\\n",
        "   .format(\"kafka\")\\\n",
        "   .option(\"kafka.bootstrap.servers\",\"127.0.0.1:9092\")\\\n",
        "   .option(\"subscribe\",topic)\\\n",
        "   .load()\n",
        "#the df sensor stream is for sensor location from which we will take location,latitude,longitude and the sensor id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I4damWj84up"
      },
      "outputs": [],
      "source": [
        "sensor_location_schema=StructType([\n",
        "StructField(\"sensor_id\", IntegerType(), True),\n",
        "StructField(\"longitude\", FloatType(), True),\n",
        "StructField(\"location\", StringType(), True),\n",
        "StructField(\"latitude\",FloatType(),True)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I07JNGSB84up"
      },
      "outputs": [],
      "source": [
        "df_sensor=df_sensor.selectExpr(\"CAST(key AS STRING)\",\"CAST(value AS STRING)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcDjjXZZ84uq"
      },
      "outputs": [],
      "source": [
        "df_sensor=df_sensor.select(F.from_json(F.col(\"value\").cast(\"string\"), sensor_location_schema).alias('parsed_value'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5I88Jvyv84uq"
      },
      "outputs": [],
      "source": [
        "df_sensor = df_sensor.select(\n",
        "                    F.col(\"parsed_value.sensor_id\").alias(\"sensor_id\"),\n",
        "                    F.col(\"parsed_value.latitude\").alias(\"latitude\"),\n",
        "                    F.col(\"parsed_value.longitude\").alias(\"longitude\"),\n",
        "                    F.col(\"parsed_value.location\").alias(\"location\")\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wYitf0y84uq",
        "outputId": "9cb31c92-621d-43de-f7df-9aea24ce2855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- sensor_id: integer (nullable = true)\n",
            " |-- latitude: float (nullable = true)\n",
            " |-- longitude: float (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_sensor.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6XM3YCo84uq"
      },
      "outputs": [],
      "source": [
        "\n",
        "#display the data of sensor on console\n",
        "query_sensor= df_sensor \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A__Zd2rv84uq"
      },
      "outputs": [],
      "source": [
        "query_sensor.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ychtpGU84ur"
      },
      "outputs": [],
      "source": [
        "#now we join the predict dataframe and df_sensor dataframe\n",
        "joined_df = df_sensor.join( predict, df_sensor.sensor_id==predict.Sensor_ID, 'inner')\\\n",
        ".select(predict.Sensor_ID,'latitude','longitude','Hourly_Counts')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOp8qau484ur",
        "outputId": "531def54-d27f-4c51-dc72-caa571e1be11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Sensor_ID: integer (nullable = true)\n",
            " |-- latitude: float (nullable = true)\n",
            " |-- longitude: float (nullable = true)\n",
            " |-- Hourly_Counts: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joined_df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLzkm-Q284ur"
      },
      "outputs": [],
      "source": [
        "#to display the columns of joined dataframe\n",
        "query_joined = joined_df\\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUqG32WG84ur"
      },
      "outputs": [],
      "source": [
        "query_joined.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4wFh8cg84ur"
      },
      "source": [
        "# combining latitude and longitude with hourly _counts>2000\n",
        "# 2.8.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df7voN1z84ur"
      },
      "outputs": [],
      "source": [
        "#we filter the data frame based on Hourly count>2000 and\n",
        "#we get a new data frame after applying filter\n",
        "joined_df_new=joined_df.filter(col('Hourly_Counts')>2000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TGtJnnk84ur"
      },
      "outputs": [],
      "source": [
        "#Now we stream the data obtained after joining both the data frames\n",
        "#we write the joined_df_new dataframe to the memory in append mode\n",
        "#using the queryName of location_plot\n",
        "joined_stream = joined_df_new \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"location_plot\")\\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IP152Ks84us",
        "outputId": "a936f95e-6ed7-4c7b-ad74-ff9faf4ba2d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+---------+-------------+\n",
            "|Sensor_ID|  latitude|longitude|Hourly_Counts|\n",
            "+---------+----------+---------+-------------+\n",
            "|        1|-37.813496|144.96515|         2172|\n",
            "|        1|-37.813496|144.96515|         2343|\n",
            "|        1|-37.813496|144.96515|         2213|\n",
            "|       41|-37.816685| 144.9669|         2099|\n",
            "|       41|-37.816685| 144.9669|         2243|\n",
            "|       41|-37.816685| 144.9669|         2019|\n",
            "|       41|-37.816685| 144.9669|         2120|\n",
            "|       41|-37.816685| 144.9669|         2165|\n",
            "|       41|-37.816685| 144.9669|         2126|\n",
            "|       41|-37.816685| 144.9669|         2020|\n",
            "|       41|-37.816685| 144.9669|         2004|\n",
            "|       41|-37.816685| 144.9669|         2124|\n",
            "|       41|-37.816685| 144.9669|         2456|\n",
            "|       41|-37.816685| 144.9669|         2804|\n",
            "|       41|-37.816685| 144.9669|         2831|\n",
            "|       41|-37.816685| 144.9669|         2830|\n",
            "|       41|-37.816685| 144.9669|         2641|\n",
            "|       41|-37.816685| 144.9669|         2639|\n",
            "|       41|-37.816685| 144.9669|         2679|\n",
            "|       41|-37.816685| 144.9669|         2176|\n",
            "+---------+----------+---------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#to display the data in location_plot\n",
        "spark1.sql(\"select * from location_plot \").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmQnY2M584us"
      },
      "outputs": [],
      "source": [
        "#we can stop the streamining of joined_stream\n",
        "joined_stream.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3EDd2p584us"
      },
      "source": [
        "\n",
        "# streaming the data from sensor_location  i.e. joined_df_new\n",
        "# to the kafka stream\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTjHvwjR84us"
      },
      "outputs": [],
      "source": [
        "#writing back the kafka stream\n",
        "\n",
        "sensor_location=spark1.sql(\"select Sensor_ID,latitude,longitude from location_plot\")\n",
        "\n",
        "query_ds = joined_df_new \\\n",
        "       .writeStream \\\n",
        "       .format(\"kafka\") \\\n",
        "       .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
        "       .option(\"topic\", \"location_sensor\") \\\n",
        "       .option(\"checkpointLocation\", \"parquet/prediction_df/checkpoint\")\\\n",
        "       .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpKkhdOh84us"
      },
      "outputs": [],
      "source": [
        "#to stop the stream of joined_df_new data above\n",
        "query_ds.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}